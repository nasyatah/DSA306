---
title: "EDAv1.0"
output: html_document
date: "2023-09-21"
---

```{r}
# OPEN CSV FILE HERE
# transactions <- arrow::open_dataset(sources = "transactions.csv", format = "csv")
# SAVE DATASET IN PARQUET FORMAT HERE
# Create AAPL directory
# dir.create("parquet_folder")  
# write parquet file to directory
# arrow::write_dataset(transactions,format = "parquet", path = "parquet_folder",partitioning = NULL)
```

```{r}
#OPEN SAVED PARQUET FILE HERE
transactions_parquet <- arrow::open_dataset(
  sources = "parquet_folder/part-0.parquet",
  format = "parquet")

# Collect the data into a data frame
transaction_df <- transactions_parquet |>
  dplyr::collect()

```

```{r}
library(sparklyr)
library(dplyr)

# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.4.0")
```

```{r}
# Copy the transactions R data frame to Spark memory and create the R reference transaction_ref
transaction_ref <- copy_to(sc, transaction_df)    
head(transaction_ref, 4)  
```

##############################################################################################################################
1. DATA CHECKING
##############################################################################################################################

1.1 Generating summary statistics for every column - applicable to numerical variables
```{r}
#summary statistics for every column
sdf_describe(transaction_ref, cols = colnames(transaction_ref)) 
```

1.2 Checking for null values in every column
```{r}
#check the number of na values present in every column 
na_values <- transaction_ref |>
  summarise_all(~sum(as.integer(is.na(.))))
na_values
```

1.3 Checking for unique values in different columns 
```{r}
#check for unique values, can modify parameter in select() to check for unique values in other columns 
unique_values <- transaction_ref |>
  select(ProductName) |>
  distinct()

unique_values
```

1.4 Checking for negative values in the Quantity column
```{r}
transaction_ref |> 
  filter(Quantity < 0)
```

```{r}
#remove these 
transaction_ref |>
  filter(Quantity >10000) 
```


##############################################################################################################################
2. DATA CLEANING 
##############################################################################################################################

############### CLEANED DATAFRAME 1 - transaction_ref2 ######################################################################
This Spark dataframe has not been grouped, and only the raw data columns have been cleaned, can be used for EDA, data viz
- date column converted to date
- quantity < 0 dropped
- Customer No with NA dropped 
- Columns: TransactionNo, ProductNo, ProductName, Price, Quantity, CustomerNo, Country, FormattedDate
##########################################################################################################################

2.1 removing rows with NA CustomerNo, and rows with negative quantity

```{r}
transaction_clean <- transaction_ref |>
  filter(!is.na(CustomerNo) & Quantity > 0 & !is.null(CustomerNo))

```

2.2 Converting date to date format
```{r}
library(dplyr)

transaction_clean <- transaction_clean |>
    mutate(
    month = substring_index(Date, "/", 1), #anyone else can't do substring?
    day = substring_index(substring_index(Date, "/", -2), "/", 1),
    year = substring_index(Date, "/", -1)
  ) 
# Add leading zeros to month and day
transaction_clean <- transaction_clean |>
  mutate(
    month = lpad(month, 2, "0"),
    day = lpad(day, 2, "0")
  )

# Combine the formatted values to create the "yyyy/mm/dd" date
transaction_clean <- transaction_clean |>
  mutate(FormattedDate = concat(year, "-", month, "-", day)) |>
  select(-month, -day, -year, -Date)

#convert to date format from chr
transaction_clean <-transaction_clean |>
  mutate(FormattedDate = to_date(FormattedDate))

transaction_clean
```

############### CLEANED DATAFRAME 2 - transaction_ref3 ######################################################################
This Spark dataframe has been grouped - columns are transformed so that it can be used as inputs to the ML model
- group by CustomerNo, Country
- Create Recency column: How many days ago was their last purchase relative to 2019-12-31
- Create Frequency Column: Count the rows of distinct TransactionNo per customer
- Create Monetary Column: Total amount spent per customer in all their transactions with the business
- ProductName: Concatenate the values from each row per customer, represent it as a tf-idf matrix where each cell contain the TF-IDF value for a specific term in a specific document. (not done yet)
- Apply z standardization for the RFM columns (not done yet)

- Columns: CustomerNo, Recency, Frequency, Monetary, R_standardised, F_standardised, M_standardised, [tf_idf matrix]
##########################################################################################################################

2.3 Create new columns for Recency, Frequency, Monetary values and their respective standardized values
https://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp 
```{r}
ref_customer <- transaction_clean |>
  group_by(CustomerNo) |>
  summarise(
    Recency = as.numeric(datediff(max(FormattedDate), to_date("2019-12-31"))*(-1)),
    Frequency = n_distinct(TransactionNo),
    Monetary = sum(Price * Quantity),
    ConcatenatedProductNames = concat_ws(", ", collect_list(ProductName))
  )

ref_customer

```


```{r}
#standardised values
# 
#  transaction_ref3|>  
#   sdf_describe(cols = c("Recency","Frequency", "Monetary"))
# 
# rfm_stats <- transaction_ref3 |>  
#   summarize(
#     m_r = mean(Recency),
#     s_r = sd(Recency),
#     m_f = mean(Frequency),
#     s_f = sd(Frequency),
#     m_m = mean(Monetary),
#     s_m = sd(Monetary)
#   ) |>  
#   collect()
# 
# rfm_stats
# 
# transaction_ref3 <- transaction_ref3 |>  
#   mutate(stdz_recency  = (Recency - !!rfm_stats$m_r) / !!rfm_stats$s_r)
#          #stdz_freqency  = (Frequency - !!rfm_stats$m_f) / !!rfm_stats$s_f,
#          #stdz_monetary  = (Monetary - !!rfm_stats$m_m) / !!rfm_stats$s_m )  

rfm_stats <- ref_customer |>
  summarize(
    r_mean = mean(Recency), 
    r_sd = sd(Recency), 
    f_mean = mean(Frequency),
    f_sd = sd(Frequency),
    m_mean = mean(Monetary),
    m_sd = sd(Monetary)
  ) |> collect() #bring back to local r

#rfm_stats

ref_customer1 <- ref_customer |>
  mutate(R_standardized = (Recency - !!rfm_stats$r_mean) / !!rfm_stats$r_sd,
         F_standardized = (Frequency - !!rfm_stats$f_mean) / !!rfm_stats$f_sd,
         M_standardized = (Monetary - !!rfm_stats$m_mean) / !!rfm_stats$m_sd) 

ref_customer1 |> 
  sdf_describe(cols = c("R_standardized","F_standardized","M_standardized"))
```

```{r}
#regress monetary with receny and frequency --> any relationshisp
#Receny --> 
#Frequency eda --> understand the purchasing habits (primary variable)

#to regress make train and test set
ref_customer_split <- ref_customer |>  
  sdf_random_split(training = 0.8, testing = 0.2, seed = 44)

ref_customer_split_train <- ref_customer_split$training
ref_customer_split_test <- ref_customer_split$testing

fit_1 <- ref_customer_split_train |> 
  ml_linear_regression(formula = Monetary ~ Recency)

fit_2 <- ref_customer_split_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency)

fit_3 <- ref_customer_split_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency + Recency) 

fit_4 <- ref_customer_split_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency + Recency + Frequency*Recency) 

fit_1$summary$r2adj #0.01847029
fit_2$summary$r2adj #0.362241
fit_3$summary$r2adj #0.3633167
fit_4$summary$r2adj #0.3651117

#the r^2 indicates that frequncy is a much more significant predictor of monetary than recency is

```
```{r}
fit_1 |> tidy()
fit_2 |> tidy()
fit_3 |> tidy()
fit_4 |> tidy()
```

```{r}
# out of sample test with MSE
pred_1 <- ml_predict(fit_3, dataset = ref_customer_split_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "Monetary",
prediction_col = "prediction",
metric_name = "mse"
)

pred_2 <- ml_predict(fit_4, dataset = ref_customer_split_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "Monetary",
prediction_col = "prediction",
metric_name = "mse"
)

MSE_1
MSE_2
```

```{r}
#3D Graph
#transaction_ref3_train
library(rgl)
open3d()
x <- ref_customer_split_train$M_standardized
y <- ref_customer_split_train$R_standardized
z <- ref_customer_split_train$F_standardized

# Create the 3D plot
plot3d(x, y, z, col = "blue", xlab = "Monetary", ylab = "Recency", zlab = "Frequency", main = "3D Plot")
```

##############################################################################################################################
3. EDA AND DATA VISUALIZATION
##############################################################################################################################
Unique_customers table
```{r}
unique_customers <- transaction_clean |>
  group_by(CustomerNo, Country) |> #finds out which country is each customer from
  summarise(total_products_bought = n_distinct(ProductNo), 
            total_spent = sum(Price*Quantity),
            total_transactions = n())|>
  collect()

unique_customers
```

```{r}
#countries with the most customers 
library(dbplot)
library(ggplot2)


dbplot_bar(unique_customers, x = Country, Customers = n_distinct(CustomerNo))
```

```{r}
customer_base <- unique_customers |> 
  group_by(Country) |>
  summarise(Customers = n_distinct(CustomerNo)) |> 
  na.omit(customer_base) |>
  filter(Customers > 20) |> #tryna cut down the data base, filtering out countries with "too low" customer bases
  collect()

dbplot_bar(customer_base, x = Country, Customers) + labs(title = "Customers per country", subtitle = "Identifying countries with the largest customer base")

```

```{r}
#country by customerbase (number of customers)
#same graph as above, but with map (think this is better cos i didnt omit data)

library(rworldmap)

customer_base1 <- unique_customers |> 
  group_by(Country) |>
  summarise(Customers = n_distinct(CustomerNo)) |> 
  na.omit(customer_base) |>
  collect()

c1 <- joinCountryData2Map(customer_base1, joinCode="NAME", nameJoinColumn="Country")
mapCountryData(c1, nameColumnToPlot="Customers", catMethod="logFixedWidth", colourPalette = "heat", addLegend = TRUE, mapTitle = "Customerbase per country")

#zoom zoom
map_region <- "Europe"
c1_europe <- mapCountryData(c1, nameColumnToPlot="Customers", mapRegion = "Europe", addLegend=FALSE, mapTitle = "Customers in Europe Region")

#from this we can see that idk i think its france/germany that has a high customer data base im bad at geography

```

```{r}
#country by expenditure 

customerbase2 <- unique_customers |> 
  group_by(Country) |> 
  summarise(Expenditure = sum(total_spent)) |> 
  collect()

c2 <- joinCountryData2Map(customerbase2, joinCode="NAME", nameJoinColumn="Country")
mapCountryData(c2, nameColumnToPlot="Expenditure", catMethod="fixedWidth", colourPalette = "heat", addLegend = TRUE, mapTitle = "Expenditure per country")

c2_europe <- mapCountryData(c2, nameColumnToPlot="Expenditure", mapRegion = "Europe", addLegend=FALSE, mapTitle = "Expenditure in Europe Region")

```

```{r}
#correlation matrix 

library(corrr)

average_spend_per_transaction <- transaction_clean |>
  group_by(TransactionNo) |>
  summarise(average_spend = sum(Price * Quantity) / n())

average_time_between_orders <- transaction_clean |>
  arrange(CustomerNo, FormattedDate) |>
  group_by(CustomerNo) |>
  summarise(average_time_between_orders = mean(diff(FormattedDate)))

unique_products_bought <- transaction_clean |>
  group_by(CustomerNo) |>
  summarise(unique_products = n_distinct(ProductName))

month_with_most_spending <- transaction_clean |>
  mutate(month = month(FormattedDate)) |>
  group_by(CustomerNo, month) |>
  summarise(total_spent = sum(Price * Quantity)) |>
  arrange(desc(total_spent)) 

#added an additional column for months with most # of orders
month_with_most_orders <- transaction_clean |>
  mutate(month = month(FormattedDate)) |>
  group_by(month) |>
  summarize(total_orders = n()) |>
  arrange(desc(total_orders))

#put all data frames into list
df <- list(average_spend_per_transaction, average_time_between_orders, unique_products_bought, month_with_most_spending)      

#merge all data frames together
corr_analysis <- Reduce(function(x, y) full_join(x, y, by = 'CustomerNo'), df)


ml_corr(corr_analysis)

```

```{r}
# Load required libraries
library(sparklyr)
library(dbplyr)
library(dbplot)
library(fitdistrplus)
library(ggplot2)

# 1. Feature Engineering

product_rfm <- transaction_clean |>
  dplyr::mutate(Revenue = Quantity * Price) |>
  group_by(ProductNo, ProductName) |>
  summarize(
    Recency = as.numeric(datediff(max(FormattedDate), to_date("2019-12-31"))*(-1)),
    Frequency = n(),
    Monetary = sum(Revenue)
  ) |>
  arrange(desc(Monetary), desc(Frequency)) |> collect()

glimpse(product_rfm)

# 2. Pareto Analysis

revenue_threshold <- sum(product_rfm$Monetary) * 0.8

top_products <- product_rfm |>
  mutate(CumulativeRevenue = cumsum(Monetary)) |>
  filter(CumulativeRevenue <= revenue_threshold) |> 
  arrange(desc(Monetary), desc(Frequency), Recency) |> collect()

top_products

# Visualize the Pareto distribution of top products
dbplot_bar(top_products, x = ProductName, y = Monetary) 

## After visualization, recognition of Pareto distribution - below are tests to confirm

# 3. Empirical Verification using Pareto Principle
cumulative_revenue <- sum(product_rfm$Monetary)
total_revenue <- sum(product_rfm$Monetary)
top_20_percent_products <- sum(cumulative_revenue <= (0.8 * total_revenue))
cat("Percentage of products contributing to 80% of revenue:", (top_20_percent_products / nrow(product_rfm)) * 100, "%\n")

# 4. T-test Verification
# Note: T-test requires data collection to R as Spark doesn't natively support this test
top_product_local <- collect(top_products)
rest_of_product_local <- collect(product_rfm) %>% 
  filter(!ProductNo %in% top_product_local$ProductNo)

t_test_result <- t.test(top_product_local$Monetary, rest_of_product_local$Monetary)
cat("T-test results comparing top products to the rest:\n")
print(t_test_result)

# Visualization for T-test
means <- data.frame(
  Group = c("Top Products", "Rest of the Products"),
  Mean = c(mean(top_product_local$Monetary), mean(rest_of_product_local$Monetary))
)

dbplot_bar(means, x=Group, y=Mean)

# 5. Kolmogorov-Smirnov Test
# Estimating parameters for Pareto distribution
xmin <- min(product_rfm$Monetary)
alpha <- length(product_rfm$Monetary) / sum(log(product_rfm$Monetary/xmin))

# Theoretical Pareto CDF function
ppareto <- function(q, alpha, xmin) {
  ifelse(q >= xmin, 1 - (xmin/q)^alpha, 0)
}

# Perform one-sample Kolmogorov-Smirnov test
ks_result <- ks.test(product_rfm$Monetary, ppareto, alpha, xmin)

print(ks_result)
```
```{r}
category_revenue <- transaction_clean |>
  dplyr::mutate(Revenue = Price*Quantity) |>
  dplyr::group_by(ProductName) |>
  dplyr::summarize(TotalRevenue = sum(Revenue)) |>
  dplyr::arrange(desc(TotalRevenue))

# Plotting
ggplot(category_revenue, aes(x=reorder(ProductName, -TotalRevenue), y=TotalRevenue)) +
  geom_bar(stat="identity", fill="skyblue") +
  labs(title="Revenue by Product Category", x="Product Category", y="Total Revenue") +
  theme_minimal() +
  theme(axis.text.x=element_blank())
```

```{r}
rfm_stats_product <- product_rfm |>
  summarize(
    r_mean = mean(Recency), 
    r_sd = sd(Recency), 
    f_mean = mean(Frequency),
    f_sd = sd(Frequency),
    m_mean = mean(Monetary),
    m_sd = sd(Monetary))

product_rfm_stdz <- product_rfm %>%
  mutate(isTopProduct = ifelse(ProductNo %in% top_products$ProductNo, 1, 0)) |>
  mutate(R_standardized = (Recency - !!rfm_stats_product$r_mean) / !!rfm_stats_product$r_sd,
         F_standardized = (Frequency - !!rfm_stats_product$f_mean) / !!rfm_stats_product$f_sd,
         M_standardized = (Monetary - !!rfm_stats_product$m_mean) / !!rfm_stats_product$m_sd) 
```

####seasonality analysis####
```{r}
#granularity must go down to the monthly basis
#is the diference between the number of transactions (peaks vs adj months) against time signifcant?

table1 <- transaction_clean |> 
  mutate(month = Month(FormattedDate)) |>
  group_by(month) |>
  summarize(Number_of_transactions = n_distinct(TransactionNo)) |>
  collect()

table1 |>
  arrange(month)

lm_fit1 <- lm(month ~ Number_of_transactions, data = table1)
summary(lm_fit1)

BIC(lm_fit1)

#plot number of transactions against months
library(ggplot2)

ggplot(table1, aes(x = month, y = Number_of_transactions)) +
  geom_line() + 
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +
  labs(x = "Month", y = "Number of Transactions") + 
  theme_minimal()

##--------------------------------------------------------------------------------------------------------------
#November has the highest number of transactions - 2753
##--------------------------------------------------------------------------------------------------------------

#in that particular month --> what is the composition of products --> which is the top 5 and bottom 5 
table2 <- transaction_clean |> 
  filter(Month(FormattedDate) == 11) |>
  group_by(ProductName, ProductNo) |>
  summarize(count = n()) |>
  collect()

table2 |> 
  arrange(desc(count))
  

##--------------------------------------------------------------------------------------------------------------
#Top 5 in November -> Rabbit Night Light, Paper Chain Kit 50'S Christmas, Hot Water Bottle Keep Calm, Paper Chain Kit Vintage Christmas, Jumbo Bag 50'S Christmas
#Bottom 5 in November -> Cake Plate Lovebird White, I'm On Holiday Metal Sign, Vintage Union Jack Memoboard, Yellow Giant Garden Thermometer, Blue Happy Birthday Bunting
##--------------------------------------------------------------------------------------------------------------

#in those products, which is the one with the greatest increase from last month (test p-value) --> demand for speeific 

#qty sold in month == 10
table3 <- transaction_clean |>
  filter(Month(FormattedDate) == 10) |>
  group_by(ProductName, ProductNo) |>
  summarize(count_prev_month = n()) |>
  collect()

table31 <- table3 |>
    filter(ProductName %in% c("Rabbit Night Light", "Paper Chain Kit 50'S Christmas", "Hot Water Bottle Keep Calm", "Paper Chain Kit Vintage Christmas", "Jumbo Bag 50'S Christmas"))

table2 <- table2 |>
  filter(ProductName %in% c("Rabbit Night Light", "Paper Chain Kit 50'S Christmas", "Hot Water Bottle Keep Calm", "Paper Chain Kit Vintage Christmas", "Jumbo Bag 50'S Christmas")) |>
  collect()

#combine table 2 and 3
table4 <- merge(table31, table2, by= c("ProductNo","ProductName"))

table4 |>
  mutate(change_from_prev_month = count - count_prev_month) |>
  arrange(desc(change_from_prev_month))

```

#Visualize the top 5 and bottom 5 products in terms of sales volume
```{r}
#group the products and sum the quantity for every product, then arrange in descending order based on quantity
ranked_products <- transaction_clean |>
  filter(Quantity >= 0 & !is.na(CustomerNo)) |>
  group_by(ProductName,ProductNo) |>
  summarise(Quantity_sold = sum(Quantity)) |>
  arrange(desc(Quantity_sold)) |>
  collect()

#choose the top 5 rows
top_5_products <- head(ranked_products,5)
#choose the bottom 5 rows
bottom_5_products <- tail(ranked_products, 5)

top_5_products
bottom_5_products

#visualise top 5
ggplot(top_5_products, aes(x = reorder(ProductName, -Quantity_sold), y = Quantity_sold)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +  # Make it a horizontal bar chart
  labs(x = "Product Name", y = "Total Quantity Sold") +
  ggtitle("Top 5 Products by Total Quantity Sold") + 
  theme_minimal()

#visualise bottom 5
ggplot(bottom_5_products, aes(x = reorder(ProductName, -Quantity_sold), y = Quantity_sold)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +  # Make it a horizontal bar chart
  labs(x = "Product Name", y = "Total Quantity Sold") +
  ggtitle("Bottom 5 Products by Total Quantity Sold") + 
  theme_minimal()

```

#Quantity sold of the top 5 products throughout the year - checking for seasonality 
```{r}
############################################################################################
#can change this based on which products/countries we wanna analyse
products_to_analyse = c("22197","84077","85099B","84879","21212")
country_to_analyse = "United Kingdom"
############################################################################################

#This dataframe groups the data by month, product and countrry, and the values for Price is aggregated to get the Average price since one item can have multiple different prices, and quantity is aggregated with the sum function. 
different_months <- transaction_clean |>
  filter(ProductNo %in% products_to_analyse,
         Country == country_to_analyse) |>
  mutate(month_sold = Month(FormattedDate)) |>
  group_by(ProductNo, ProductName, Country, month_sold) |> 
  summarise(Quantity = sum(Quantity),
            Avg_Price = mean(Price)) |>
  collect()

#View the collected Spark dataframe
different_months 

#Visualising the quantity sold every month for the top 5 products
ggplot(different_months, aes(x = month_sold, y = Quantity, color = ProductName)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +  # Set numeric months and labels
  labs(x = "Month", y = "Quantity Sold") +
  ggtitle("Quantity Sold per Month for Selected Products") +
  theme(legend.position = "top") + 
  theme_minimal()

#Visualising the change in average prices of the products every month
ggplot(different_months, aes(x = month_sold, y = Avg_Price, color = ProductName)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +  # Set numeric months and labels
  labs(x = "Month", y = "Average Price") +
  ggtitle("Average Price per Month for Selected Products") +
  theme(legend.position = "top") + 
  theme_minimal()

###INSIGHTS####
#Prices for most products dip in October which is also when there is a spike in Quantity of products sold. However some #products such as World War 2 Gliders Asstd Designs see a drop in sales in October even with a drop in price. This data can be #used to assist managers in their pricing strategies for the various products. 


#if price drops from our optimal basket, does it actually increase sales --> quanitiy disc
```


##############################################################################################################################
4. MODELING IN SPARK
##############################################################################################################################



##############################################################################################################################
5. SPARK ML PIPELINE
##############################################################################################################################

```{r}
#test
#converting transaction_ref3 into an ML pipeline

# 2.3 data cleaning converted to ML pipeline
pipeline <- ml_pipeline(
  ft_group_by(transaction_clean, CustomerNo),
  ft_summarise(
    Recency = as.numeric(datediff(max(transaction_clean$FormattedDate), to_date("2019-12-31")) * (-1)),
    Frequency = n_distinct(transaction_clean$TransactionNo),
    Monetary = sum(transaction_clean$Price * transaction_clean$Quantity),
    ConcatenatedProductNames = concat_ws(", ", collect_list(transaction_clean$ProductName))
  )
)

#Creating a model to fit pipeline into the data?
model <- ml_fit(pipeline, transaction_clean)

# Transform the data using the pipeline
ref_customer <- ml_transform(model, transaction_clean)

# Displaying the result
sdf_register(ref_customer, "ref_customer")

```

