---
title: "EDAv1.0"
output: html_document
date: "2023-09-21"
---

```{r}
# OPEN CSV FILE HERE
# transactions <- arrow::open_dataset(sources = "transactions.csv", format = "csv")
# SAVE DATASET IN PARQUET FORMAT HERE
# Create AAPL directory
# dir.create("parquet_folder")  
# write parquet file to directory
# arrow::write_dataset(transactions,format = "parquet", path = "parquet_folder",partitioning = NULL)
```

```{r}
#OPEN SAVED PARQUET FILE HERE
transactions_parquet <- arrow::open_dataset(
  sources = "parquet_folder/part-0.parquet",
  format = "parquet")

# Collect the data into a data frame
transaction_df <- transactions_parquet |>
  dplyr::collect()

```

```{r}
library(sparklyr)
library(dplyr)

# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.4.0")
```

```{r}
# Copy the transactions R data frame to Spark memory and create the R reference transaction_ref
transaction_ref <- copy_to(sc, transaction_df)    
head(transaction_ref, 4)  
```

##############################################################################################################################
1. DATA CHECKING
##############################################################################################################################

1.1 Generating summary statistics for every column - applicable to numerical variables
```{r}
#summary statistics for every column
sdf_describe(transaction_ref, cols = colnames(transaction_ref)) 
```

1.2 Checking for null values in every column
```{r}
#check the number of na values present in every column 
na_values <- transaction_ref |>
  summarise_all(~sum(as.integer(is.na(.))))
na_values
```

1.3 Checking for unique values in different columns 
```{r}
#check for unique values, can modify parameter in select() to check for unique values in other columns 
unique_values <- transaction_ref |>
  select(ProductName) |>
  distinct() |>
  collect()

unique_values
```



1.4 Checking for negative values in the Quantity column
```{r}
transaction_ref |> 
  filter(Quantity < 0)
```


##############################################################################################################################
2. DATA CLEANING 
##############################################################################################################################

############### CLEANED DATAFRAME 1 - transaction_ref2 ######################################################################
This Spark dataframe has not been grouped, and only the raw data columns have been cleaned, can be used for EDA, data viz
- date column converted to date
- quantity < 0 dropped
- Customer No with NA dropped 
- Columns: TransactionNo, ProductNo, ProductName, Price, Quantity, CustomerNo, Country, FormattedDate
##########################################################################################################################

2.1 removing rows with NA CustomerNo, and rows with negative quantity

```{r}
transaction_ref2 <- transaction_ref |>
  filter(!is.na(CustomerNo) & Quantity > 0 & !is.null(CustomerNo)) #|>
  #collect()

#USE TRANS DF FOR ALL FUTURE ANALYSIS??
```

2.2 Converting date to date format
```{r}
library(dplyr)

transaction_ref2 <- transaction_ref2 |>
    mutate(
    month = substring_index(Date, "/", 1), #anyone else can't do substring?
    day = substring_index(substring_index(Date, "/", -2), "/", 1),
    year = substring_index(Date, "/", -1)
  ) 
# Add leading zeros to month and day
transaction_ref2 <- transaction_ref2 |>
  mutate(
    month = lpad(month, 2, "0"),
    day = lpad(day, 2, "0")
  )

# Combine the formatted values to create the "yyyy/mm/dd" date
transaction_ref2 <- transaction_ref2 |>
  mutate(FormattedDate = concat(year, "-", month, "-", day)) |>
  select(-month, -day, -year, -Date)

#convert to date format from chr
transaction_ref2 <-transaction_ref2 |>
  mutate(FormattedDate = to_date(FormattedDate))

transaction_ref2
```
############### CLEANED DATAFRAME 2 - transaction_ref3 ######################################################################
This Spark dataframe has been grouped - columns are transformed so that it can be used as inputs to the ML model
- group by CustomerNo, Country
- Create Recency column: How many days ago was their last purchase relative to 2019-12-31
- Create Frequency Column: Count the rows of distinct TransactionNo per customer
- Create Monetary Column: Total amount spent per customer in all their transactions with the business
- ProductName: Concatenate the values from each row per customer, represent it as a tf-idf matrix where each cell contain the TF-IDF value for a specific term in a specific document. (not done yet)
- Apply z standardization for the RFM columns (not done yet)

- Columns: CustomerNo, Recency, Frequency, Monetary, R_standardised, F_standardised, M_standardised, [tf_idf matrix]
##########################################################################################################################

2.3 Create new columns for Recency, Frequency, Monetary values and their respective standardized values
https://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp 
```{r}
transaction_ref3 <- transaction_ref2 |>
  group_by(CustomerNo) |>
  summarise(
    Recency = as.numeric(datediff(max(FormattedDate), to_date("2019-12-31"))*(-1)),
    Frequency = n_distinct(TransactionNo),
    Monetary = sum(Price * Quantity),
    ConcatenatedProductNames = concat_ws(", ", collect_list(ProductName))
  )

transaction_ref3

```


```{r}
#standardised values
# 
#  transaction_ref3|>  
#   sdf_describe(cols = c("Recency","Frequency", "Monetary"))
# 
# rfm_stats <- transaction_ref3 |>  
#   summarize(
#     m_r = mean(Recency),
#     s_r = sd(Recency),
#     m_f = mean(Frequency),
#     s_f = sd(Frequency),
#     m_m = mean(Monetary),
#     s_m = sd(Monetary)
#   ) |>  
#   collect()
# 
# rfm_stats
# 
# transaction_ref3 <- transaction_ref3 |>  
#   mutate(stdz_recency  = (Recency - !!rfm_stats$m_r) / !!rfm_stats$s_r)
#          #stdz_freqency  = (Frequency - !!rfm_stats$m_f) / !!rfm_stats$s_f,
#          #stdz_monetary  = (Monetary - !!rfm_stats$m_m) / !!rfm_stats$s_m )  

rfm_stats <- transaction_ref3 |>
  summarize(
    r_mean = mean(Recency), 
    r_sd = sd(Recency), 
    f_mean = mean(Frequency),
    f_sd = sd(Frequency),
    m_mean = mean(Monetary),
    m_sd = sd(Monetary)
  ) |> collect()

#rfm_stats

transaction_ref3_1 <- transaction_ref3 |>
  mutate(R_standardized = (Recency - !!rfm_stats$r_mean) / !!rfm_stats$r_sd,
         F_standardized = (Frequency - !!rfm_stats$f_mean) / !!rfm_stats$f_sd,
         M_standardized = (Monetary - !!rfm_stats$m_mean) / !!rfm_stats$m_sd) 

transaction_ref3_1 |> 
  sdf_describe(cols = c("R_standardized","F_standardized","M_standardized"))
```

```{r}
#regress monetary with receny and frequency --> any relationshisp
#Receny --> 
#Frequency eda --> understand the purchasing habits (primary variable)

#to regress make train and test set
transaction_ref3_split <- transaction_ref3 |>  
  sdf_random_split(training = 0.8, testing = 0.2, seed = 44)

transaction_ref3_train <- transaction_ref3_split$training
transaction_ref3_test <- transaction_ref3_split$testing

fit_1 <- transaction_ref3_train |> 
  ml_linear_regression(formula = Monetary ~ Recency)

fit_2 <- transaction_ref3_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency)

fit_3 <- transaction_ref3_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency + Recency) 

fit_4 <- transaction_ref3_train |> 
  ml_linear_regression(formula = Monetary ~ Frequency + Recency + Frequency*Recency) 

fit_1$summary$r2adj #0.01847029
fit_2$summary$r2adj #0.362241
fit_3$summary$r2adj #0.3633167
fit_4$summary$r2adj #0.3651117

#the r^2 indicates that frequncy is a much more significant predictor of monetary than recency is

```
```{r}
fit_1 |> tidy()
fit_2 |> tidy()
fit_3 |> tidy()
fit_4 |> tidy()
```

```{r}
# out of sample test with MSE
pred_1 <- ml_predict(fit_3, dataset = transaction_ref3_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "Monetary",
prediction_col = "prediction",
metric_name = "mse"
)

pred_2 <- ml_predict(fit_4, dataset = transaction_ref3_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "Monetary",
prediction_col = "prediction",
metric_name = "mse"
)

MSE_1
MSE_2
```

```{r}
#3D Graph
#transaction_ref3_train
library(rgl)
x <- collect(transaction_ref3_train$M_standardized)
y <- collect(transaction_ref3_train$R_standardized)
z <- collect(transaction_ref3_train$F_standardized)
transaction_ref3_train |> plot3d( x,y,z, col = "blue", xlab = "Monetary", ylab = "Recency", zlab = "Frequency", main = "3D Plot")

```

##############################################################################################################################
3. EDA AND DATA VISUALIZATION
##############################################################################################################################
Unique_customers table
```{r}

unique_customers <- transaction_ref2 |>
  group_by(CustomerNo, Country) |> #finds out which country is each customer from
  summarise(total_products_bought = n_distinct(ProductNo), 
            total_spent = sum(Price*Quantity),
            total_transactions = n())|>
  collect()

```

```{r}
#countries with the most customers 
library(dbplot)
library(ggplot2)


dbplot_bar(unique_customers, x = Country, Customers = n_distinct(CustomerNo))
```

```{r}
#grouping the data by customer id and transaction date and get the count and then see how many rows have a count >1

sameday_transasctions <- transaction_ref2 |>
  group_by(CustomerNo, FormattedDate) |>
  summarise(total_products_bought = n_distinct(ProductNo), 
            total_spent = sum(Price*Quantity),
            total_transactions = n())

sameday_transasctions <- sameday_transasctions |>
  filter(total_transactions> 1)
sameday_transasctions

num_customers <- sameday_transasctions |>
  group_by(FormattedDate) |>
  summarise(num_of_customers = n_distinct(CustomerNo))
num_customers |> collect()

dbplot_bar(num_customers, x = FormattedDate,  y= n_distinct(CustomerNo))
```

```{r}
customer_base <- unique_customers |> 
  group_by(Country) |>
  summarise(Customers = n_distinct(CustomerNo)) |> 
  na.omit(customer_base) |>
  filter(Customers > 20) |> #tryna cut down the data base, filtering out countries with "too low" customer bases
  collect()

dbplot_bar(customer_base, x = Country, Customers) + labs(title = "Customers per country", subtitle = "Identifying countries with the largest customer base")
```

```{r}
#country by customerbase (number of customers)
#same graph as above, but with map (think this is better cos i didnt omit data)

library(rworldmap)

customer_base1 <- unique_customers |> 
  group_by(Country) |>
  summarise(Customers = n_distinct(CustomerNo)) |> 
  na.omit(customer_base) |>
  collect()

c1 <- joinCountryData2Map(customer_base1, joinCode="NAME", nameJoinColumn="Country")
mapCountryData(c1, nameColumnToPlot="Customers", catMethod="logFixedWidth", colourPalette = "heat", addLegend = TRUE, mapTitle = "Customerbase per country")

#zoom zoom
map_region <- "Europe"
c1_europe <- mapCountryData(c1, nameColumnToPlot="Customers", mapRegion = "Europe", addLegend=FALSE, mapTitle = "Customers in Europe Region")

#from this we can see that idk i think its france/germany that has a high customer data base im bad at geography

```

```{r}
#country by expenditure 

customerbase2 <- unique_customers |> 
  group_by(Country) |> 
  summarise(Expenditure = sum(total_spent)) |> 
  collect()

c2 <- joinCountryData2Map(customerbase2, joinCode="NAME", nameJoinColumn="Country")
mapCountryData(c2, nameColumnToPlot="Expenditure", catMethod="fixedWidth", colourPalette = "heat", addLegend = TRUE, mapTitle = "Expenditure per country")

c2_europe <- mapCountryData(c2, nameColumnToPlot="Expenditure", mapRegion = "Europe", addLegend=FALSE, mapTitle = "Expenditure in Europe Region")

```

```{r}
#product analysis by understanding the revenue contribution is pareto distributed

library(sparklyr)
library(dbplyr)
library(dbplot)

# 1. Feature Engineering
transaction_ref2_spark <- transaction_ref2_spark %>%
  mutate(Revenue = Price * Quantity)

product_rfm <- transaction_ref2_spark %>%
  group_by(ProductNo, ProductName) %>%
  summarise(
    Recency = as.numeric(difftime(max(as.Date("2019-09-12")), max(as.Date(FormattedDate, format="%Y-%m-%d")), units = "days")),
    Frequency = n(),
    Monetary = sum(Revenue)
  ) %>%
  arrange(desc(Monetary), desc(Frequency), Recency)

# 2. Pareto Analysis
product_rfm <- product_rfm %>%
  mutate(CumulativeRevenue = cumsum(Monetary))

revenue_threshold <- sum(product_rfm$Monetary) * 0.8
top_products <- product_rfm %>%
  filter(CumulativeRevenue <= revenue_threshold)

# Visualize the Pareto distribution of top products
db_plot(top_products, ProductName, Monetary) +
  labs(title="Top Products by Revenue", x="Product Name", y="Revenue")

# 3. Empirical Verification using Pareto Principle
cumulative_revenue <- sum(product_rfm$Monetary)
total_revenue <- sum(product_rfm$Monetary)
top_20_percent_products <- sum(cumulative_revenue <= 0.8 * total_revenue)
cat("Percentage of products contributing to 80% revenue:", (top_20_percent_products / nrow(product_rfm)) * 100, "%\n")

# 4. T-test Verification
# Note: T-test requires data collection to R as Spark doesn't natively support this test
top_product_local <- collect(top_products)
rest_of_product_local <- collect(product_rfm) %>% 
  filter(!ProductNo %in% top_product_local$ProductNo)
t_test_result <- t.test(top_product_local$Monetary, rest_of_product_local$Monetary)
cat("T-test results comparing top products to the rest:\n")
print(t_test_result)

# Visualization for T-test
means <- data.frame(
  Group = c("Top Products", "Rest of the Products"),
  Mean = c(top_product_means, rest_of_product_means)
)
ggplot(means, aes(x=Group, y=Mean)) +
  geom_bar(stat="identity") +
  labs(title="Mean Monetary Value Comparison", x="", y="Mean Monetary Value")

# 5. Kolmogorov-Smirnov Test
fit <- fitdist(top_products$Monetary, "pareto")
ks_results <- gofstat(fit)
print(ks_results$kstest)
summary(fit)

# Visualization for KS Test
plot(fit)
```

```{r}
#granularity must go down to the monthly basis
#is the diference between the number of transactions (peaks vs adj months) against time signifcant?

table1 <- transaction_ref2 |> 
  mutate(month = Month(FormattedDate)) |>
  group_by(month) |>
  summarize(Number_of_transactions = n_distinct(TransactionNo)) |>
  collect()

table1 |>
  arrange(month)

lm_fit1 <- lm(month ~ Number_of_transactions, data = table1)
summary(lm_fit1)

BIC(lm_fit)

#plot number of transactions against months
library(ggplot2)

ggplot(table1, aes(x = month, y = Number_of_transactions)) +
  geom_line() + 
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +
  labs(x = "Month", y = "Number of Transactions") + 
  theme_minimal()

##--------------------------------------------------------------------------------------------------------------
#November has the highest number of transactions - 2753
##--------------------------------------------------------------------------------------------------------------

#in that particular month --> what is the composition of products --> which is the top 5 and bottom 5 
table2 <- transaction_ref2 |> 
  filter(Month(FormattedDate) == 11) |>
  group_by(ProductName, ProductNo) |>
  summarize(count = n()) |>
  collect()

table2 |> 
  arrange(desc(count))
  

##--------------------------------------------------------------------------------------------------------------
#Top 5 in November -> Rabbit Night Light, Paper Chain Kit 50'S Christmas, Hot Water Bottle Keep Calm, Paper Chain Kit Vintage Christmas, Jumbo Bag 50'S Christmas
#Bottom 5 in November -> Cake Plate Lovebird White, I'm On Holiday Metal Sign, Vintage Union Jack Memoboard, Yellow Giant Garden Thermometer, Blue Happy Birthday Bunting
##--------------------------------------------------------------------------------------------------------------

#in those products, which is the one with the greatest increase from last month (test p-value) --> demand for speeific 

#qty sold in month == 10
table3 <- transaction_ref2 |>
  filter(Month(FormattedDate) == 10) |>
  group_by(ProductName, ProductNo) |>
  summarize(count_prev_month = n()) |>
  collect()

table31 <- table3 |>
    filter(ProductName %in% c("Rabbit Night Light", "Paper Chain Kit 50'S Christmas", "Hot Water Bottle Keep Calm", "Paper Chain Kit Vintage Christmas", "Jumbo Bag 50'S Christmas"))

table2 <- table2 |>
  filter(ProductName %in% c("Rabbit Night Light", "Paper Chain Kit 50'S Christmas", "Hot Water Bottle Keep Calm", "Paper Chain Kit Vintage Christmas", "Jumbo Bag 50'S Christmas")) |>
  collect()

#combine table 2 and 3
table4 <- merge(table31, table2, by= c("ProductNo","ProductName"))

table4 |>
  mutate(change_from_prev_month = count - count_prev_month) |>
  arrange(desc(change_from_prev_month))

  


```

#Visualize the top 5 and bottom 5 products in terms of sales volume
```{r}
#group the products and sum the quantity for every product, then arrange in descending order based on quantity
ranked_products <- transaction_ref2 |>
  filter(Quantity >= 0 & !is.na(CustomerNo)) |>
  group_by(ProductName,ProductNo) |>
  summarise(Quantity_sold = sum(Quantity)) |>
  arrange(desc(Quantity_sold)) |>
  collect()

#choose the top 5 rows
top_5_products <- head(ranked_products,5)
#choose the bottom 5 rows
bottom_5_products <- tail(ranked_products, 5)

top_5_products
bottom_5_products

#visualise top 5
ggplot(top_5_products, aes(x = reorder(ProductName, -Quantity_sold), y = Quantity_sold)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +  # Make it a horizontal bar chart
  labs(x = "Product Name", y = "Total Quantity Sold") +
  ggtitle("Top 5 Products by Total Quantity Sold") + 
  theme_minimal()

#visualise bottom 5
ggplot(bottom_5_products, aes(x = reorder(ProductName, -Quantity_sold), y = Quantity_sold)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +  # Make it a horizontal bar chart
  labs(x = "Product Name", y = "Total Quantity Sold") +
  ggtitle("Bottom 5 Products by Total Quantity Sold") + 
  theme_minimal()

```

#Quantity sold of the top 5 products throughout the year - checking for seasonality 
```{r}
############################################################################################
#can change this based on which products/countries we wanna analyse
products_to_analyse = c("22197","84077","85099B","84879","21212")
country_to_analyse = "United Kingdom"
############################################################################################

#This dataframe groups the data by month, product and countrry, and the values for Price is aggregated to get the Average price since one item can have multiple different prices, and quantity is aggregated with the sum function. 
different_months <- transaction_ref2 |>
  filter(ProductNo %in% products_to_analyse,
         Country == country_to_analyse) |>
  mutate(month_sold = Month(FormattedDate)) |>
  group_by(ProductNo, ProductName, Country, month_sold) |> 
  summarise(Quantity = sum(Quantity),
            Avg_Price = mean(Price)) |>
  collect()

#View the collected Spark dataframe
different_months 

#Visualising the quantity sold every month for the top 5 products
ggplot(different_months, aes(x = month_sold, y = Quantity, color = ProductName)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +  # Set numeric months and labels
  labs(x = "Month", y = "Quantity Sold") +
  ggtitle("Quantity Sold per Month for Selected Products") +
  theme(legend.position = "top") + 
  theme_minimal()

#Visualising the change in average prices of the products every month
ggplot(different_months, aes(x = month_sold, y = Avg_Price, color = ProductName)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +  # Set numeric months and labels
  labs(x = "Month", y = "Average Price") +
  ggtitle("Average Price per Month for Selected Products") +
  theme(legend.position = "top") + 
  theme_minimal()

###INSIGHTS####
#Prices for most products dip in October which is also when there is a spike in Quantity of products sold. However some #products such as World War 2 Gliders Asstd Designs see a drop in sales in October even with a drop in price. This data can be #used to assist managers in their pricing strategies for the various products. 


#if price drops from our optimal basket, does it actually increase sales --> quanitiy disc
```

##############################################################################################################################
4. MODELING IN SPARK
##############################################################################################################################



##############################################################################################################################
5. SPARK ML PIPELINE
##############################################################################################################################
