---
title: "DSA306_final"
output: html_document
date: "2023-11-11"
---

##------------------------
#SECTION 1: Big Data Apache File Format
##------------------------
#Reading the csv file and saving it as Parquet format
```{r}
# OPEN CSV FILE HERE
# transactions <- arrow::open_dataset(sources = "transactions.csv", format = "csv")
# SAVE DATASET IN PARQUET FORMAT HERE
# Create AAPL directory
# dir.create("parquet_folder")  
# write parquet file to directory
# arrow::write_dataset(transactions,format = "parquet", path = "parquet_folder",partitioning = NULL)
```

#Opening the parquet file and create a reference in Spark
```{r}
#OPEN SAVED PARQUET FILE HERE
transactions_parquet <- arrow::open_dataset(
  sources = "parquet_folder/part-0.parquet",
  format = "parquet")

# Collect the data into a data frame
transaction_df <- transactions_parquet |>
  dplyr::collect()


library(sparklyr)
library(dplyr)

# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.4.0")

# Copy the transactions R data frame to Spark memory and create the R reference transaction_ref
transaction_ref <- copy_to(sc, transaction_df)    
head(transaction_ref, 4)  
```

##------------------------
#SECTION 2.1: Data Checking
##------------------------

2.1.1 Generating summary statistics for every column - applicable to numerical variables
```{r}
#summary statistics for every column
sdf_describe(transaction_ref, cols = colnames(transaction_ref)) 
```

2.1.2 Checking for null values in every column
```{r}
#check the number of na values present in every column 
na_values <- transaction_ref |>
  summarise_all(~sum(as.integer(is.na(.))))
na_values
```

2.1.3 Checking for unique values in different columns 
```{r}
#check for unique values, can modify parameter in select() to check for unique values in other columns 
unique_values <- transaction_ref |>
  select(ProductName) |>
  distinct()

unique_values
```

2.1.4 Checking for negative values in the Quantity column
```{r}
transaction_ref |> 
  filter(Quantity < 0)
```

2.1.5 Checking for outliers where Quantity is extremely large
```{r}
transaction_ref |>
  filter(Quantity >10000) 
```

##------------------------
#SECTION 2.2: Data Cleaning
##------------------------

2.2.1 removing rows with NA CustomerNo, and rows with negative quantity

```{r}
transaction_clean <- transaction_ref |>
  filter(!is.na(CustomerNo) & (Quantity > 0 & Quantity < 10000) & !is.null(CustomerNo))

```

2.2.2 Converting date to date format
```{r}
library(dplyr)

transaction_clean <- transaction_clean |>
    mutate(
    month = substring_index(Date, "/", 1), 
    day = substring_index(substring_index(Date, "/", -2), "/", 1),
    year = substring_index(Date, "/", -1)
  ) 
# Add leading zeros to month and day
transaction_clean <- transaction_clean |>
  mutate(
    month = lpad(month, 2, "0"),
    day = lpad(day, 2, "0")
  )

# Combine the formatted values to create the "yyyy/mm/dd" date
transaction_clean <- transaction_clean |>
  mutate(FormattedDate = concat(year, "-", month, "-", day)) |>
  dplyr::select(-month, -day, -year, -Date)

#convert to date format from chr
transaction_clean <-transaction_clean |>
  mutate(FormattedDate = to_date(FormattedDate))

transaction_clean
```
2.2.3 Removing transactions from 2018 - focus on only 2019 data
```{r}
transaction_clean <- transaction_clean |>
  filter(year(FormattedDate) == 2019)

```

2.2.4 Filtering for only UK customers (Run this code after running 3.1)
```{r}
transaction_clean <- transaction_clean |>
  filter(Country == "United Kingdom")

```

##------------------------
#SECTION 3: Customer EDA and Visualization
##------------------------

3.1 Geographical Analysis
```{r}
#countries with the most customers 
library(dbplot)
library(ggplot2)
country_plot <- transaction_clean |> 
  mutate(Country = ifelse(Country == "United Kingdom", Country, "Others"))

dbplot_bar(country_plot, x = Country, Customers = n_distinct(CustomerNo)) + 
  theme_minimal()

#after geographical an
```

3.2 Visualizing transaction volume by day of the week
```{r}
#which day of the week has the most transactions? 
# Extract the day of the week as an integer (1 for Sunday, 2 for Monday, etc.)
temp_df <- transaction_clean |>
  mutate(day_of_week = dayofweek(FormattedDate))

temp_df <- temp_df |>
    group_by(day_of_week) |>
    summarise(number_of_transactions = n_distinct(TransactionNo)) |>
  collect()

#no transactions on Tuesdays -  add a column for Tuesday , where number_of_transactions = 0
tuesday <- list(day_of_week = 3, number_of_transactions = 0)
temp_df <- rbind(temp_df, tuesday)

#checking for statistical signifance
lm_fit <- lm(number_of_transactions ~ day_of_week, data = temp_df)
summary(lm_fit)

day_names <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

# Create a plot
ggplot(temp_df, aes(x = factor(day_of_week), y = number_of_transactions)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Day of the Week", y = "Number of Transactions") +
  scale_x_discrete(labels = day_names) +
  theme_minimal()
```

3.3 Visualizing transaction volume by month
```{r}
temp_df1 <- transaction_clean |> 
  mutate(month = Month(FormattedDate)) |>
  group_by(month) |>
  summarize(Number_of_transactions = n_distinct(TransactionNo)) |>
  collect()

#test for statistical significance
lm_fit <- lm(month ~ Number_of_transactions, data = temp_df1)
summary(lm_fit)

#plot number of transactions against months
ggplot(temp_df1, aes(x = month, y = Number_of_transactions)) +
  geom_line() + 
  scale_x_continuous(breaks = 1:12, labels = month.abb[1:12]) +
  labs(x = "Month", y = "Number of Transactions") + 
  theme_minimal()

temp_df1
```

3.4 Average spend per transaction
```{r}
#To check for high variablity, look at standard deviation or IQR
temp_df2 <- transaction_clean |>
  group_by(TransactionNo) |>
  summarize(avg_spend = sum(Price*Quantity)/n_distinct(TransactionNo))

sdf_describe(temp_df2, cols = "avg_spend")

temp_df2 <- temp_df2 |>
  filter(avg_spend < 25000) |>
  collect() #collect back to r for visualization purposes

#Plot
ggplot(temp_df2, aes(x = avg_spend)) +
  geom_histogram(binwidth = 1000, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Average Spend per Transaction", x = "Average Spend", y = "Frequency") +
  theme_minimal()



```

3.5 Number of unique products bought per customer
```{r}
#get standard deviation in spark 
temp_df3 <- transaction_clean |>
  group_by(CustomerNo) |>
  summarize(no_of_unique_products = n_distinct(ProductNo))

sdf_describe(temp_df3, cols = "no_of_unique_products")

temp_df3 <- temp_df3 |>
  filter(no_of_unique_products < 500) |>
  collect()

ggplot(temp_df3, aes(x = no_of_unique_products)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of unique products bought per customer", x = "Number of Unique Products", y = "Frequency") + 
  theme_minimal()


```

3.6 Average Basket Size
```{r}

#use sdf_describe
temp_df4 <- transaction_clean |>
  group_by(CustomerNo) |>
  summarize(avg_basket_size = sum(Quantity)/n_distinct(TransactionNo))

sdf_describe(temp_df4, cols = "avg_basket_size")

#Collect data back into r for visualization
temp_df4 <- temp_df4 |>
  filter(avg_basket_size < 5000) |>
  collect()

#Plot
ggplot(temp_df4, aes(x = avg_basket_size)) +
  geom_histogram(binwidth = 50, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Average Basket Size per customer", x = "Average Basket Size", y = "Frequency") + 
  theme_minimal()

```

##------------------------
#SECTION 4: Feature Engineering 
##------------------------

4.1 Feature engineering for ref_customer
```{r}
ref_customer <- transaction_clean |>
  group_by(CustomerNo) |>
  summarise(
    Recency = as.numeric(datediff(to_date("2019-12-09"), max(FormattedDate))),
    Frequency = n_distinct(TransactionNo),
    Monetary = sum(Price * Quantity),
    Duration = as.numeric(datediff(max(FormattedDate), min(FormattedDate))),
    Unique_products = n_distinct(ProductName), 
    Average_basket_size = sum(Quantity)/n_distinct(TransactionNo), 
    Avg_spend_per_transaction = sum(Price*Quantity)/n_distinct(TransactionNo)
    ) 

#create a column to identify the month where the each customer spent the most
temp_df <- transaction_clean |> 
  select(CustomerNo, FormattedDate, Price,Quantity) |>
  mutate(total_spent = Price*Quantity) |>
  group_by(CustomerNo, month(FormattedDate)) |>
  summarise(total_spent_per_month = sum(total_spent)) |>
  group_by(CustomerNo) |>
  filter(total_spent_per_month == max(total_spent_per_month)) |>
  group_by(CustomerNo) |>
  mutate(rank = row_number(total_spent_per_month)) |>
  filter(rank == 1) |>
  rename(month_with_max_spending = "month(FormattedDate)") |>
  select(CustomerNo, month_with_max_spending)

#left_join the customer_ref with the month_with_max_spending column in temp_df
ref_customer <- left_join(ref_customer, temp_df, by = "CustomerNo")

#recode the values in month_with_max_spending to 0 or 1, where customers who spent the most in Oct-Nov will be assigned 1 and the rest are assigned 0. This feature will be used to idenitfy festive spenders. 
ref_customer <- ref_customer |>
  mutate(month_with_max_spending = ifelse(month_with_max_spending >= 10, "1", "0")) |>
  rename(festive_spender = month_with_max_spending)


#to create target variable, find the median duration
median_duration <- ref_customer |>
  summarize(median_duration = median(Duration)) |>
  collect()

#column for target variable - target engineering 
ref_customer <- ref_customer |>
  mutate(logistic_duration = ifelse(Duration > !!median_duration$median_duration, 1, 0))

ref_customer
```

##------------------------
#SECTION 5.1: Modelling in Spark - Logistic Regression
##------------------------

5.1.1 Correlation Analysis
```{r}
#correlation matrix
library(corrr)
library(ggplot2)
ref_customer |>
  select(-CustomerNo, -Duration, -festive_spender, -logistic_duration) |>
  correlate(use = "pairwise.complete.obs", method = "pearson")


#plot the sample correlations
corr_plot <- ref_customer |> 
  select(-CustomerNo, -Duration, -festive_spender, -logistic_duration) |>
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) 

corr_plot2 <- rplot(corr_plot)

# Modify the theme settings to adjust the x-axis labels
corr_plot2 + theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Remove average spend per transaction(Monetary/Frequency) since it is highly correlated with average basket size
```
5.1.2 Splitting into training and testing set
```{r}
ref_customer_split <- ref_customer |>  
  sdf_random_split(training = 0.8, testing = 0.2, seed = 44)

ref_customer_split_train <- ref_customer_split$training
ref_customer_split_test <- ref_customer_split$testing

ref_customer_split_train
```

5.1.3 Standardisation of variables 
```{r}
#standardised values

#get the mean and sd values based on the training set
ref_customer_stats <- ref_customer_split_train |>
  summarize(
    r_mean = mean(Recency), r_sd = sd(Recency), 
    f_mean = mean(Frequency), f_sd = sd(Frequency),
    m_mean = mean(Monetary), m_sd = sd(Monetary), 
    unique_product_mean = mean(Unique_products), unique_product_sd = sd(Unique_products), 
    avg_basket_size_mean = mean(Average_basket_size), avg_basket_size_sd = sd(Average_basket_size)
  ) |> collect() #bring back to local r

#apply it to the training set
ref_customer_split_train <- ref_customer_split_train |>
  mutate(R_standardized = (Recency - !!ref_customer_stats$r_mean) / !!ref_customer_stats$r_sd,
         F_standardized = (Frequency - !!ref_customer_stats$f_mean) / !!ref_customer_stats$f_sd,
         M_standardized = (Monetary - !!ref_customer_stats$m_mean) / !!ref_customer_stats$m_sd, 
         Unique_products_standardized = (Unique_products - !!ref_customer_stats$unique_product_mean) / !!ref_customer_stats$unique_product_sd,
         Average_basket_size_standardized = (Average_basket_size - !!ref_customer_stats$avg_basket_size_mean) / !!ref_customer_stats$avg_basket_size_sd) 

#apply it to the test set
ref_customer_split_test <- ref_customer_split_test |>
  mutate(R_standardized = (Recency - !!ref_customer_stats$r_mean) / !!ref_customer_stats$r_sd,
         F_standardized = (Frequency - !!ref_customer_stats$f_mean) / !!ref_customer_stats$f_sd,
         M_standardized = (Monetary - !!ref_customer_stats$m_mean) / !!ref_customer_stats$m_sd, 
         Unique_products_standardized = (Unique_products - !!ref_customer_stats$unique_product_mean) / !!ref_customer_stats$unique_product_sd,
         Average_basket_size_standardized = (Average_basket_size - !!ref_customer_stats$avg_basket_size_mean) / !!ref_customer_stats$avg_basket_size_sd) 
ref_customer_split_train |>
  sdf_describe(cols = c("R_standardized","F_standardized","M_standardized","Unique_products_standardized","Average_basket_size_standardized"))

```

5.1.4 CI plot - with all the variables 
```{r}
tidy_glm_fit <- ref_customer_split_train |>
  ml_generalized_linear_regression(
    formula = logistic_duration ~ M_standardized + F_standardized + R_standardized + Average_basket_size_standardized + Unique_products_standardized + festive_spender
  ) |> tidy() |> collect()

#CI Plot
library(ggplot2)

tidy_glm_fit |>
  ggplot(aes(x = term, y = estimate)) + geom_point(size = 0.5) + geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.1 )+
  geom_hline(yintercept = 0, linetype = "dashed") + 
  coord_flip() +
  labs(title = "Confidence Intervals",
       subtitle = "Parameter estimates with approximate 95% confidence intervals" )

```


5.1.5 Evaluating the models
```{r}

#only rfm
fit_logistic1<- ref_customer_split_train |> 
  ml_logistic_regression(formula = logistic_duration ~ M_standardized + F_standardized + R_standardized)

validation_summary1 <- ml_evaluate(fit_logistic1, dataset = ref_customer_split_test)
validation_summary1$area_under_roc()

fit_logistic1

#all the variables except avg_spend_per_transaction
fit_logistic2 <- ref_customer_split_train |>
  ml_logistic_regression(formula = logistic_duration ~ M_standardized + F_standardized + R_standardized + Average_basket_size_standardized + Unique_products_standardized + festive_spender)

validation_summary2 <- ml_evaluate(fit_logistic2, dataset = ref_customer_split_test)
validation_summary2$area_under_roc()

fit_logistic2

```

5.1.5 Out-of-sample testing (remove?)
```{r}
# out of sample test with MSE
pred_1 <- ml_predict(fit_logistic1, dataset = ref_customer_split_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "logistic_duration",
prediction_col = "prediction",
metric_name = "mse"
)

pred_2 <- ml_predict(fit_logistic2, dataset = ref_customer_split_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "logistic_duration",
prediction_col = "prediction",
metric_name = "mse"
)


MSE_1
MSE_2

```


##------------------------
#SECTION 5.2: ML Pipeline - Logistic Regression
##------------------------

5.2.1 Logistic Regression ML Pipeline
```{r}
#Logistic_pipeline <- ml_pipeline(sc)
#ML Pipeline Logistic Regression
Logistic_pipeline <- ml_pipeline(sc) |>
  ft_string_indexer(
    input_col = "festive_spender",
    output_col = "festive_spender_indexed"
    ) |>
  ft_one_hot_encoder(
    input_col = "festive_spender_indexed",
    output_col = "festive_spender_encoded"
  ) |>
  ft_vector_assembler(
    input_cols = c("Monetary", "Frequency", "Recency","Average_basket_size","Unique_products"),
    output_col = "continuous_features"
  ) |>
  ft_standard_scaler(
    input_col = "continuous_features",
    output_col = "stdz_continuous_features",
    with_mean = TRUE
  ) |>
    ft_vector_assembler(
    input_cols = c("stdz_continuous_features","festive_spender_encoded"),
    output_col = "all_features"
  ) |>
  ml_logistic_regression(
    features_col = "all_features",
    label_col = "logistic_duration"
  )

Logistic_pipeline

```

5.2.2 Cross validation 
```{r}
#cross-validation with elastic net
cv <- ml_cross_validator(
  sc,
  estimator = Logistic_pipeline,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75,1), #alpha
      reg_param = c(0, 0.001, 0.01, 0.1) #lambda
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "logistic_duration"
  ),
  num_folds = 10,
  parallelism = 4,
  seed = 1337
)

cv_model <- ml_fit(cv, ref_customer_split_train) #training model or the full set?

ml_validation_metrics(cv_model) |>
  arrange(desc(areaUnderROC))

#save the pipeline model to disk
ml_save(cv_model$best_model, path = "spark_model1", overwrite = TRUE)
```
5.2.3 Deploy model to production using Plumber
```{r}
#install.packages("plumber")
library(plumber)

#Start a Web Service
plumb(file = "spark-plumber1.R") |>
  pr_run(port = 8000)

```


##------------------------
#SECTION 5.3: Modelling in Spark - Kmeans Clustering
##------------------------

5.3.1 Determining the optimal number of clusters 
```{r}
#iterate through the different values of k 
k_values <- c(2, 3, 4, 5, 6)
silhouette_scores <- numeric(length(k_values))

for (i in 1:length(k_values)) {
  k <- k_values[i]
  
  k_means_model <- ref_customer |>
    ml_kmeans(
      formula = ~ R_standardized + F_standardized + M_standardized,
      k = k, 
      max_iter = 1000, 
      init_mode = "random",
      seed = 8472
    )
  #calculate silhouette scores
  silhouette_scores[i] <- ml_compute_silhouette_measure(
  k_means_model,
  dataset = ref_customer,
  distance_measure = "squaredEuclidean"
)
  
}

#find the best value of k - plot silhouette score against k 
library(ggplot2)

# Create a data frame for k values and silhouette scores
silhouette_data <- data.frame(k = k_values, silhouette_score = silhouette_scores)

silhouette_data

ggplot(silhouette_data, aes(k,silhouette_score)) +
  geom_line() +
  theme_minimal()

#as shown in the plot k = 4 is the optimal number of clusters 
```


##------------------------
#SECTION 5.4: ML Pipeline - Kmeans Clustering
##------------------------

5.4.1 Creating the pipeline, fitting the model and collecting the predictions
```{r}
kmeans_pipeline <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = c("Recency","Frequency","Monetary"),
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "features_stdz",
    with_mean = TRUE
  ) |>
  ml_kmeans(
    features_col = "features_stdz",
    prediction_col = "cluster",
    k=4,
    max_iter = 1000,
    init_mode = "random",
    seed = 8472
  ) 
  
fitted_model <- ml_fit(kmeans_pipeline, ref_customer)



#save the pipeline model to disk
ml_save(fitted_model, path = "spark_model2", overwrite = TRUE)
```

5.4.2 Deploy Kmeans model to production using Plumber
```{r}
library(plumber)

#Start a Web Service
plumb(file = "spark-plumber2.R") |>
  pr_run(port = 8000)

```


5.4.3 Understanding the clusters
```{r}
#Every customer is assigned to one of the 4 clusters - 0,1,2,3
#What do these clusters mean? 
predictions <- ml_transform(fitted_model, ref_customer) |>
  collect()
#move the dataframe with all the ref_customer columns tgt with the cluster column into spark to do further analysis
ref_prediction <- copy_to(sc, predictions, overwrite = TRUE)

#group by the cluster and get the mean values of RFM 
ref_prediction |>
  group_by(cluster) |>
  summarise(
    mean_recency = mean(Recency), 
    mean_frequency = mean(Frequency), 
    mean_monetary = mean(Monetary)
  ) |>
  arrange(cluster)
```

5.4.4 Visualizing the clusters
```{r}
#install.packages("scatterplot3d")
library(scatterplot3d)

recency <- unlist(lapply(predictions$features_stdz, function(x) x[[1]]))
frequency <- unlist(lapply(predictions$features_stdz, function(x) x[[2]]))
monetary <- unlist(lapply(predictions$features_stdz, function(x) x[[3]]))

predictions$cluster <- as.factor(predictions$cluster)

cluster_colors <- c("red", "blue", "green","yellow")
colors <- cluster_colors[predictions$cluster]

# Create the 3D scatter plot
scatterplot3d(recency, frequency, monetary, color = colors)
```



